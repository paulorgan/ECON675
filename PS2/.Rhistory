fhat_li <- rowMeans(df)
# apply kernel function to x-x_i
df <- matrix((1/h)*K0((Xi_n-X)/h),nrow=M)
# repeat observation for each simulation
Xi_n <- matrix(rep(X[,i],n), nrow=M)
# apply kernel function to x-x_i
df1 <- K0((Xi_n-X)/h)
View(df)
View(df1)
X <- matrix(dgp(n),nrow=M,ncol=n)
View(X)
gc()
###############################################################################
# Author: Paul R. Organ
# Purpose: ECON 675, PS2
# Last Update: Oct 7, 2018
###############################################################################
# Preliminaries
options(stringsAsFactors = F)
# packages
require(tidyverse) # data cleaning and manipulation
require(magrittr)  # syntax
require(ggplot2)   # plots
require(kedd)      # kernel bandwidth estimation
require(car)       # heteroskedastic robust SEs
require(xtable)    # tables for LaTeX
setwd('C:/Users/prorgan/Box/Classes/Econ 675/Problem Sets/PS2')
###############################################################################
## Question 1: Kernel Density Estimation
## Q1.3a
# sample size
n     <- 1000
# data generating process
dgp <- function(n){
# equally weight two distributions
comps <- sample(1:2,prob=c(.5,.5),size=n,replace=T)
# Normal density specs
mus <- c(-1.5, 1)
sds <- sqrt(c(1.5, 1))
# generate sample
samp <- rnorm(n=n,mean=mus[comps],sd=sds[comps])
return(samp)
}
# mean of mixture
mu_true <- .5*-1.5 + .5*1
# sd of mixture
var_true <- .5*1.5 + .5*1 + (.5*-1.5+.5*1-(.5*-1.5+.5*1)^2)
# see: https://stats.stackexchange.com/questions/16608/
# what-is-the-variance-of-the-weighted-mixture-of-two-gaussians
# true dgp
f_true <- function(x){dnorm(x,mean=mu_true,sd=sqrt(var_true))}
# second deriv of normal dist
norm_2d <- function(u,meanu,sdu){dnorm(u,mean=meanu,sd=sdu)*
(((u-meanu)^2/(sdu^4))-(1/(sdu^2)))}
# AIMSE-optimal bandwidth choice
optimal_h <- function(n,mean,sd){
f <- function(x,mean,sd){norm_2d(x,mean,sd)^2}
k1 <- .75^2*(2-4/3+2/5)
k2 <- .75*(2/3-2/5)
k3 <- integrate(f, lower = -Inf, upper = Inf, m = mean, s = sd)$val
h <- (k1/(k3*k2^2)*(1/n))^(1/5)
return(h)
}
# theoretically optimal h
h_aimse <- optimal_h(1000,mu_true,sqrt(var_true))
###############################################################################
## Q1.3b
# define Kernel function: K(u)=.75(1-u^2)(ind(abs(u)<=1))
K0 <- function(u){
out <- .75 * (1-u^2) * (abs(u) <= 1)
}
# function to calculate IMSE
imse <- function(h,X){
# empty matrices to fill with results
e_li <- matrix(NA,nrow=M,ncol=n)
e_lo <- matrix(NA,nrow=M,ncol=n)
# loop over each i to do leave one out
for(i in 1:n){
# repeat observation for each simulation
Xi_n <- matrix(rep(X[,i],n), nrow=M)
# apply kernel function to x-x_i
df1 <- K0((Xi_n-X)/h)
df <- matrix((1/h)*K0((Xi_n-X)/h),nrow=M)
# fhat with i in
fhat_li <- rowMeans(df)
# fhat with i out
fhat_lo <- rowMeans(df[,-i])
# f(x_i)
f_xi <- f_true(X[,i])
# mse with i in
e_li[,i] <- (fhat_li-f_xi)^2
# mse with i out
e_lo[,i] <- (fhat_lo-f_xi)^2
}
# take mean over M reps and i obs
out <- c(mean(e_li),mean(e_lo))
return(out)
}
# simulate 1000 times
M <- 1000
M <- 100
# generate matrix with M rows of sampled data
set.seed(22)
X <- matrix(dgp(n),nrow=M,ncol=n)
# sequence of h's to test
hs <- seq(.5,1.5,.1) * h_aimse
# loop through values of h and calculate mean IMSE with i in, out
ptm <- proc.time()
means <- lapply(hs, X, FUN = imse)
proc.time() - ptm
means_100 <- means
M <- 200
set.seed(22)
X <- matrix(dgp(n),nrow=M,ncol=n)
# sequence of h's to test
hs <- seq(.5,1.5,.1) * h_aimse
# loop through values of h and calculate mean IMSE with i in, out
ptm <- proc.time()
means <- lapply(hs, X, FUN = imse)
proc.time() - ptm
means_200 <- means
A <- matrix(c(1,2),c(1,2))
A <- matrix(c(1,2,3,4),nrow=2,ncol=2)
mean(A)
M <- 50
set.seed(22)
X <- matrix(dgp(n),nrow=M,ncol=n)
h<-hs[6]
e_li <- matrix(NA,nrow=M,ncol=n)
e_lo <- matrix(NA,nrow=M,ncol=n)
View(e_li)
i<-1
# repeat observation for each simulation
Xi_n <- matrix(rep(X[,i],n), nrow=M)
View(X)
View(Xi_n)
# apply kernel function to x-x_i
df1 <- K0((Xi_n-X)/h)
# apply kernel function to x-x_i
df1 <- (1/h)*K0((Xi_n-X)/h)
df <- matrix((1/h)*K0((Xi_n-X)/h),nrow=M)
View(df)
View(df1)
# apply kernel function to x-x_i
df <- (1/h)*K0((Xi_n-X)/h)
# fhat with i in
fhat_li <- rowMeans(df)
# fhat with i out
fhat_lo <- rowMeans(df[,-i])
# f(x_i)
f_xi <- f_true(X[,i])
# mse with i in
e_li[,i] <- (fhat_li-f_xi)^2
# mse with i out
e_lo[,i] <- (fhat_lo-f_xi)^2
e_li[,1]
imse <- function(h,X){
# empty matrices to fill with results
e_li <- matrix(NA,nrow=M,ncol=n)
e_lo <- matrix(NA,nrow=M,ncol=n)
# loop over each i to do leave one out
for(i in 1:n){
# repeat observation for each simulation
Xi_n <- matrix(rep(X[,i],n), nrow=M)
# apply kernel function to x-x_i
df <- (1/h)*K0((Xi_n-X)/h)
# fhat with i in
fhat_li <- rowMeans(df)
# fhat with i out
fhat_lo <- rowMeans(df[,-i])
# f(x_i)
f_xi <- f_true(X[,i])
# mse with i in
e_li[,i] <- (fhat_li-f_xi)^2
# mse with i out
e_lo[,i] <- (fhat_lo-f_xi)^2
}
# take mean over M reps and i obs
out <- c(mean(e_li),mean(e_lo))
return(out)
}
###############################################################################
# Author: Paul R. Organ
# Purpose: ECON 675, PS2
# Last Update: Oct 7, 2018
###############################################################################
# Preliminaries
options(stringsAsFactors = F)
# packages
require(tidyverse) # data cleaning and manipulation
require(magrittr)  # syntax
require(ggplot2)   # plots
require(kedd)      # kernel bandwidth estimation
require(car)       # heteroskedastic robust SEs
require(xtable)    # tables for LaTeX
setwd('C:/Users/prorgan/Box/Classes/Econ 675/Problem Sets/PS2')
###############################################################################
## Question 1: Kernel Density Estimation
## Q1.3a
# sample size
n     <- 1000
# data generating process
dgp <- function(n){
# equally weight two distributions
comps <- sample(1:2,prob=c(.5,.5),size=n,replace=T)
# Normal density specs
mus <- c(-1.5, 1)
sds <- sqrt(c(1.5, 1))
# generate sample
samp <- rnorm(n=n,mean=mus[comps],sd=sds[comps])
return(samp)
}
# mean of mixture
mu_true <- .5*-1.5 + .5*1
# sd of mixture
var_true <- .5*1.5 + .5*1 + (.5*-1.5+.5*1-(.5*-1.5+.5*1)^2)
# see: https://stats.stackexchange.com/questions/16608/
# what-is-the-variance-of-the-weighted-mixture-of-two-gaussians
# true dgp
f_true <- function(x){dnorm(x,mean=mu_true,sd=sqrt(var_true))}
# second deriv of normal dist
norm_2d <- function(u,meanu,sdu){dnorm(u,mean=meanu,sd=sdu)*
(((u-meanu)^2/(sdu^4))-(1/(sdu^2)))}
# AIMSE-optimal bandwidth choice
optimal_h <- function(n,mean,sd){
f <- function(x,mean,sd){norm_2d(x,mean,sd)^2}
k1 <- .75^2*(2-4/3+2/5)
k2 <- .75*(2/3-2/5)
k3 <- integrate(f, lower = -Inf, upper = Inf, m = mean, s = sd)$val
h <- (k1/(k3*k2^2)*(1/n))^(1/5)
return(h)
}
# theoretically optimal h
h_aimse <- optimal_h(1000,mu_true,sqrt(var_true))
###############################################################################
## Q1.3b
# define Kernel function: K(u)=.75(1-u^2)(ind(abs(u)<=1))
K0 <- function(u){
out <- .75 * (1-u^2) * (abs(u) <= 1)
}
# function to calculate IMSE
imse <- function(h,X){
# empty matrices to fill with results
e_li <- matrix(NA,nrow=M,ncol=n)
e_lo <- matrix(NA,nrow=M,ncol=n)
# loop over each i to do leave one out
for(i in 1:n){
# repeat observation for each simulation
Xi_n <- matrix(rep(X[,i],n), nrow=M)
# apply kernel function to x-x_i
df <- (1/h)*K0((Xi_n-X)/h)
# fhat with i in
fhat_li <- rowMeans(df)
# fhat with i out
fhat_lo <- rowMeans(df[,-i])
# f(x_i)
f_xi <- f_true(X[,i])
# mse with i in
e_li[,i] <- (fhat_li-f_xi)^2
# mse with i out
e_lo[,i] <- (fhat_lo-f_xi)^2
}
# take mean over M reps and i obs
out <- c(mean(e_li),mean(e_lo))
return(out)
}
# simulate 1000 times
M <- 1000
M <- 50
# generate matrix with M rows of sampled data
set.seed(22)
X <- matrix(dgp(n),nrow=M,ncol=n)
# sequence of h's to test
hs <- seq(.5,1.5,.1) * h_aimse
# loop through values of h and calculate mean IMSE with i in, out
ptm <- proc.time()
means_50 <- lapply(hs, X, FUN = imse)
proc.time() - ptm
M <- 100
# generate matrix with M rows of sampled data
X <- matrix(dgp(n),nrow=M,ncol=n)
# sequence of h's to test
hs <- seq(.5,1.5,.1) * h_aimse
# loop through values of h and calculate mean IMSE with i in, out
ptm <- proc.time()
means_100 <- lapply(hs, X, FUN = imse)
proc.time() - ptm
# function to calculate IMSE
imse <- function(h,X){
# empty matrices to fill with results
e_li <- matrix(NA,nrow=M,ncol=n)
e_lo <- matrix(NA,nrow=M,ncol=n)
# loop over each i to do leave one out
for(i in 1:n){
# repeat observation for each simulation
Xi_n <- matrix(rep(X[,i],n), nrow=M)
# apply kernel function to (x-x_i)/h
df <- (1/h)*K0((Xi_n-X)/h)
# fhat with i in
fhat_li <- rowMeans(df)
# fhat with i out
fhat_lo <- rowMeans(df[,-i])
# f(x_i)
f_xi <- f_true(X[,i])
# mse with i in
e_li[,i] <- (fhat_li-f_xi)^2
# mse with i out
e_lo[,i] <- (fhat_lo-f_xi)^2
}
# take mean over M reps and i obs
out <- c(mean(e_li),mean(e_lo))
return(out)
}
h <- hs[1]
M <- 20
set.seed(22)
X <- matrix(dgp(n),nrow=M,ncol=n)
# empty matrices to fill with results
e_li <- matrix(NA,nrow=M,ncol=n)
e_lo <- matrix(NA,nrow=M,ncol=n)
for(i in 1:n){
# repeat observation for each simulation
Xi_n <- matrix(rep(X[,i],n), nrow=M)
# apply kernel function to (x-x_i)/h
df <- (1/h)*K0((Xi_n-X)/h)
# fhat with i in
fhat_li <- rowMeans(df)
# fhat with i out
fhat_lo <- rowMeans(df[,-i])
# f(x_i)
f_xi <- f_true(X[,i])
# mse with i in
e_li[,i] <- (fhat_li-f_xi)^2
# mse with i out
e_lo[,i] <- (fhat_lo-f_xi)^2
}
View(e_li)
View(e_li)
mean(e_li)
# mean over i
e_lo_m <- rowMeans(e_lo)
# take mean over M reps and i obs
# mean over M
e_lo_mean <- mean(e_lo_m)
mean(e_lo)
# repeat observation for each simulation
Xi_n <- matrix(rep(X[,i],n), nrow=M)
View(Xi_n)
set.seed(22)
X <- matrix(dgp(n),nrow=M,ncol=n)
# repeat observation for each simulation
Xi_n <- matrix(rep(X[,i],n), nrow=M)
i <- 1
# repeat observation for each simulation
Xi_n <- matrix(rep(X[,i],n), nrow=M)
# apply kernel function to (x-x_i)/h
df <- (1/h)*K0((Xi_n-X)/h)
View(df)
# fhat with i in
fhat_li <- rowMeans(df)
# fhat with i out
fhat_lo <- rowMeans(df[,-i])
# f(x_i)
f_xi <- f_true(X[,i])
# mse with i in
e_li[,i] <- (fhat_li-f_xi)^2
# mse with i out
e_lo[,i] <- (fhat_lo-f_xi)^2
?matrix
gc()
###############################################################################
# Author: Paul R. Organ
# Purpose: ECON 675, PS2
# Last Update: Oct 7, 2018
###############################################################################
# Preliminaries
options(stringsAsFactors = F)
# packages
require(tidyverse) # data cleaning and manipulation
require(magrittr)  # syntax
require(ggplot2)   # plots
require(kedd)      # kernel bandwidth estimation
require(car)       # heteroskedastic robust SEs
require(xtable)    # tables for LaTeX
setwd('C:/Users/prorgan/Box/Classes/Econ 675/Problem Sets/PS2')
###############################################################################
## Question 1: Kernel Density Estimation
## Q1.3a
# sample size
n     <- 1000
# data generating process
dgp <- function(n){
# equally weight two distributions
comps <- sample(1:2,prob=c(.5,.5),size=n,replace=T)
# Normal density specs
mus <- c(-1.5, 1)
sds <- sqrt(c(1.5, 1))
# generate sample
samp <- rnorm(n=n,mean=mus[comps],sd=sds[comps])
return(samp)
}
# mean of mixture
mu_true <- .5*-1.5 + .5*1
# sd of mixture
var_true <- .5*1.5 + .5*1 + (.5*-1.5+.5*1-(.5*-1.5+.5*1)^2)
# see: https://stats.stackexchange.com/questions/16608/
# what-is-the-variance-of-the-weighted-mixture-of-two-gaussians
# true dgp
f_true <- function(x){dnorm(x,mean=mu_true,sd=sqrt(var_true))}
# second deriv of normal dist
norm_2d <- function(u,meanu,sdu){dnorm(u,mean=meanu,sd=sdu)*
(((u-meanu)^2/(sdu^4))-(1/(sdu^2)))}
# AIMSE-optimal bandwidth choice
optimal_h <- function(n,mean,sd){
f <- function(x,mean,sd){norm_2d(x,mean,sd)^2}
k1 <- .75^2*(2-4/3+2/5)
k2 <- .75*(2/3-2/5)
k3 <- integrate(f, lower = -Inf, upper = Inf, m = mean, s = sd)$val
h <- (k1/(k3*k2^2)*(1/n))^(1/5)
return(h)
}
# theoretically optimal h
h_aimse <- optimal_h(1000,mu_true,sqrt(var_true))
###############################################################################
## Q1.3b
# define Kernel function: K(u)=.75(1-u^2)(ind(abs(u)<=1))
K0 <- function(u){
out <- .75 * (1-u^2) * (abs(u) <= 1)
}
# function to calculate IMSE
imse <- function(h,X){
# empty matrices to fill with results
e_li <- matrix(NA,nrow=M,ncol=n)
e_lo <- matrix(NA,nrow=M,ncol=n)
# loop over each i to do leave one out
for(i in 1:n){
# repeat observation for each simulation
Xi_n <- matrix(rep(X[,i],n), nrow=M)
# apply kernel function to (x-x_i)/h
df <- (1/h)*K0((Xi_n-X)/h)
# fhat with i in
fhat_li <- rowMeans(df)
# fhat with i out
fhat_lo <- rowMeans(df[,-i])
# f(x_i)
f_xi <- f_true(X[,i])
# mse with i in
e_li[,i] <- (fhat_li-f_xi)^2
# mse with i out
e_lo[,i] <- (fhat_lo-f_xi)^2
}
# take mean over M reps and i obs
out <- c(mean(e_li),mean(e_lo))
return(out)
}
# simulate 1000 times
M <- 1000
# generate matrix with M rows of sampled data
set.seed(22)
X <- matrix(dgp(n),nrow=M,ncol=n)
# sequence of h's to test
hs <- seq(.5,1.5,.1) * h_aimse
# loop through values of h and calculate mean IMSE with i in, out
ptm <- proc.time()
means <- lapply(hs, X, FUN = imse)
proc.time() - ptm
976/90
df <- means %>% unlist %>% matrix(nrow = length(hs)) %>% data.frame %>%
rename(IMSE_LI = X1, IMSE_LO = X2) %>% mutate(h = hs)
View(df)
# plot
p <- ggplot(df) + geom_smooth(aes(x=h,y=IMSE_LI))
p
# plot
p <- ggplot(df) + geom_line(aes(x=h,y=IMSE_LI))
p
?geom_smooth
# plot
p <- ggplot(df) + geom_smooth(aes(x=h,y=IMSE_LI), se=F)
p
# plot
p <- ggplot(df) +
geom_smooth(aes(x=h,y=IMSE_LI), se=F) +
geom_smooth(aes(x=h,y=IMSE_LI), se=F)
p
# plot
p <- ggplot(df) +
geom_smooth(aes(x=h,y=IMSE_LI), se=F) +
geom_smooth(aes(x=h,y=IMSE_LO), se=F)
p
# plot
p <- ggplot(df) +
geom_smooth(aes(x=h,y=IMSE_LI), se=F) +
geom_smooth(aes(x=h,y=IMSE_LO), se=F) +
scale_color_manual(values=c('black','red')) +
theme(legend.position='none')
p
# format
df <- means %>% unlist %>% matrix(nrow = length(hs)) %>% data.frame %>%
rename(leavein = X1, leaveout = X2) %>% mutate(h = hs) %>%
gather(key = inout, value = imse, -h)
View(df)
# plot
p <- ggplot(df, aes(x=h,y=imse,color=type)) + geom_smooth(se=F)
p
# plot
p <- ggplot(df, aes(x=h,y=imse,color=inout)) + geom_smooth(se=F)
p
# plot
p <- ggplot(df, aes(x=h,y=imse,color=inout)) + geom_smooth(se=F) +
theme_minimal()
p
