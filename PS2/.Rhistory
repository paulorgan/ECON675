return(samp)
}
# mean of mixture
mu_true <- .5*-1.5 + .5*1
# sd of mixture
var_true <- .5*1.5 + .5*1 + (.5*-1.5+.5*1-(.5*-1.5+.5*1)^2)
# see: https://stats.stackexchange.com/questions/16608/
# what-is-the-variance-of-the-weighted-mixture-of-two-gaussians
# true dgp
f_true <- function(x){dnorm(x,mean=mu_true,sd=sqrt(var_true))}
# second deriv of normal dist
norm_2d <- function(u,meanu,sdu){dnorm(u,mean=meanu,sd=sdu)*
(((u-meanu)^2/(sdu^4))-(1/(sdu^2)))}
# AIMSE-optimal bandwidth choice
optimal_h <- function(n,mean,sd){
f <- function(x,mean,sd){norm_2d(x,mean,sd)^2}
k1 <- .75^2*(2-4/3+2/5)
k2 <- .75*(2/3-2/5)
k3 <- integrate(f, lower = -Inf, upper = Inf, m = mean, s = sd)$val
h <- (k1/(k3*k2^2)*(1/n))^(1/5)
return(h)
}
# theoretically optimal h
h_aimse <- optimal_h(1000,mu_true,sqrt(var_true))
###############################################################################
## Q1.3b
# define Kernel function: K(u)=.75(1-u^2)(ind(abs(u)<=1))
K0 <- function(u){
out <- .75 * (1-u^2) * (abs(u) <= 1)
}
# function to calculate IMSE
imse <- function(h,X){
# empty matrices to fill with results
e_li <- matrix(NA,nrow=M,ncol=n)
e_lo <- matrix(NA,nrow=M,ncol=n)
# loop over each i to do leave one out
for(i in 1:n){
# repeat observation for each simulation
Xi_n <- matrix(rep(X[,i],n), nrow=M)
# apply kernel function to (x-x_i)/h
df <- (1/h)*K0((Xi_n-X)/h)
# fhat with i in
fhat_li <- rowMeans(df)
# fhat with i out
fhat_lo <- rowMeans(df[,-i])
# f(x_i)
f_xi <- f_true(X[,i])
# mse with i in
e_li[,i] <- (fhat_li-f_xi)^2
# mse with i out
e_lo[,i] <- (fhat_lo-f_xi)^2
}
# take mean over M reps and i obs
out <- c(mean(e_li),mean(e_lo))
return(out)
}
# simulate 1000 times
M <- 1000
# generate matrix with M rows of sampled data
set.seed(22)
X <- matrix(dgp(n),nrow=M,ncol=n)
# sequence of h's to test
hs <- seq(.5,1.5,.1) * h_aimse
# loop through values of h and calculate mean IMSE with i in, out
ptm <- proc.time()
means <- lapply(hs, X, FUN = imse)
proc.time() - ptm
# runtime 11 minutes
# my code is wrong somewhere - the average IMSE increases with M; it should not
# format
df <- means %>% unlist %>% matrix(nrow = length(hs)) %>% data.frame %>%
rename(leavein = X1, leaveout = X2) %>% mutate(h = hs) %>%
gather(key = inout, value = imse, -h)
# plot
p <- ggplot(df, aes(x=h,y=imse,color=inout)) + geom_smooth(se=F) +
theme_minimal()
p
# simulate 1000 times
M <- 100
set.seed(22)
X <- matrix(dgp(n),nrow=M,ncol=n)
# sequence of h's to test
hs <- seq(.5,1.5,.1) * h_aimse
# loop through values of h and calculate mean IMSE with i in, out
ptm <- proc.time()
means <- lapply(hs, X, FUN = imse)
proc.time() - ptm
# format
df <- means %>% unlist %>% matrix(nrow = length(hs)) %>% data.frame %>%
rename(leavein = X1, leaveout = X2) %>% mutate(h = hs) %>%
gather(key = inout, value = imse, -h)
p <- ggplot(df, aes(x=h,y=imse,color=inout)) + geom_smooth(se=F) +
theme_minimal()
p
M <- 10
# generate matrix with M rows of sampled data
set.seed(22)
X <- matrix(dgp(n),nrow=M,ncol=n)
# sequence of h's to test
hs <- seq(.5,1.5,.1) * h_aimse
# loop through values of h and calculate mean IMSE with i in, out
ptm <- proc.time()
means <- lapply(hs, X, FUN = imse)
proc.time() - ptm
imse <- function(h,X){
# empty matrices to fill with results
e_li <- rep(NA,n)
e_lo <- rep(NA,n)
# loop over each i to do leave one out
for(i in 1:n){
# repeat observation for each simulation
Xi_n <- rep(X[i], n)
# apply kernel function to (x-x_i)/h
df <- (1/h)*K0((Xi_n-X)/h)
# fhat with i in
fhat_li <- mean(df)
# fhat with i out
fhat_lo <- mean(df[,-i])
# f(x_i)
f_xi <- f_true(X)
# mse with i in
e_li[i] <- (fhat_li-f_xi)^2
# mse with i out
e_lo[i] <- (fhat_lo-f_xi)^2
}
out <- c(mean(e_li),mean(e_lo))
return(out)
}
# simulate 1000 times
M <- 10
# sequence of h's to test
hs <- seq(.5,1.5,.1) * h_aimse
nh <- length(hs)
# empty matrices
IMSE_LI <- matrix(NA, nrow=M, ncol=nh)
IMSE_LO <- matrix(NA, nrow=M, ncol=nh)
# generate matrix with M rows of sampled data
set.seed(22)
for(m in 1:M){
X <- dgp(n)
for(j in 1:nh){
temp <- imse(hs[j],X)
IMSE_LI[m,j] <- temp[1]
IMSE_L0[m,j] <- temp[2]
}
}
# simulate 1000 times
M <- 10
# sequence of h's to test
hs <- seq(.5,1.5,.1) * h_aimse
nh <- length(hs)
# empty matrices
IMSE_LI <- matrix(NA, nrow=M, ncol=nh)
IMSE_LO <- matrix(NA, nrow=M, ncol=nh)
m <- 1
X <- dgp(n)
j <- 1
temp <- imse(hs[j],X)
# empty matrices to fill with results
e_li <- rep(NA,n)
e_lo <- rep(NA,n)
i <- 1
# repeat observation for each simulation
Xi_n <- rep(X[i], n)
# apply kernel function to (x-x_i)/h
df <- (1/h)*K0((Xi_n-X)/h)
h <- hs[]
h <- hs[1]
# apply kernel function to (x-x_i)/h
df <- (1/h)*K0((Xi_n-X)/h)
# fhat with i in
fhat_li <- mean(df)
# fhat with i out
fhat_lo <- mean(df[,-i])
# fhat with i out
fhat_lo <- mean(df[-i])
imse <- function(h,X){
# empty matrices to fill with results
e_li <- rep(NA,n)
e_lo <- rep(NA,n)
# loop over each i to do leave one out
for(i in 1:n){
# repeat observation for each simulation
Xi_n <- rep(X[i], n)
# apply kernel function to (x-x_i)/h
df <- (1/h)*K0((Xi_n-X)/h)
# fhat with i in
fhat_li <- mean(df)
# fhat with i out
fhat_lo <- mean(df[-i])
# f(x_i)
f_xi <- f_true(X)
# mse with i in
e_li[i] <- (fhat_li-f_xi)^2
# mse with i out
e_lo[i] <- (fhat_lo-f_xi)^2
}
out <- c(mean(e_li),mean(e_lo))
return(out)
}
# simulate 1000 times
M <- 10
# sequence of h's to test
hs <- seq(.5,1.5,.1) * h_aimse
nh <- length(hs)
# empty matrices
IMSE_LI <- matrix(NA, nrow=M, ncol=nh)
IMSE_LO <- matrix(NA, nrow=M, ncol=nh)
# generate matrix with M rows of sampled data
set.seed(22)
for(m in 1:M){
X <- dgp(n)
for(j in 1:nh){
temp <- imse(hs[j],X)
IMSE_LI[m,j] <- temp[1]
IMSE_L0[m,j] <- temp[2]
}
}
warnings()
# f(x_i)
f_xi <- f_true(X)
# f(x_i)
f_xi <- f_true(X[i])
imse <- function(h,X){
# empty matrices to fill with results
e_li <- rep(NA,n)
e_lo <- rep(NA,n)
# loop over each i to do leave one out
for(i in 1:n){
# repeat observation for each simulation
Xi_n <- rep(X[i], n)
# apply kernel function to (x-x_i)/h
df <- (1/h)*K0((Xi_n-X)/h)
# fhat with i in
fhat_li <- mean(df)
# fhat with i out
fhat_lo <- mean(df[-i])
# f(x_i)
f_xi <- f_true(X[i])
# mse with i in
e_li[i] <- (fhat_li-f_xi)^2
# mse with i out
e_lo[i] <- (fhat_lo-f_xi)^2
}
out <- c(mean(e_li),mean(e_lo))
return(out)
}
# simulate 1000 times
M <- 10
# sequence of h's to test
hs <- seq(.5,1.5,.1) * h_aimse
nh <- length(hs)
# empty matrices
IMSE_LI <- matrix(NA, nrow=M, ncol=nh)
IMSE_LO <- matrix(NA, nrow=M, ncol=nh)
# generate matrix with M rows of sampled data
set.seed(22)
for(m in 1:M){
X <- dgp(n)
for(j in 1:nh){
temp <- imse(hs[j],X)
IMSE_LI[m,j] <- temp[1]
IMSE_LO[m,j] <- temp[2]
}
}
df <- data.frame(h = hs, leavein = IMSE_LI, leaveout = IMSE_LO) %>%
gather(key = inout, value = imse, -h)
View(IMSE_LI)
df <- data.frame(h = hs, leavein = colMeans(IMSE_LI), leaveout = colMeans(IMSE_LO)) %>%
gather(key = inout, value = imse, -h)
View(df)
df <- means %>% unlist %>% matrix(nrow = length(hs)) %>% data.frame %>%
rename(leavein = X1, leaveout = X2) %>% mutate(h = hs) %>%
gather(key = inout, value = imse, -h)
df2 <- data.frame(h = hs, leavein = colMeans(IMSE_LI), leaveout = colMeans(IMSE_LO)) %>%
gather(key = inout, value = imse, -h)
View(df2)
View(df)
# plot
p <- ggplot(df2, aes(x=h,y=imse,color=inout)) + geom_smooth(se=F) +
theme_minimal()
p
df2_10 <- df2
# simulate 1000 times
M <- 20
# sequence of h's to test
hs <- seq(.5,1.5,.1) * h_aimse
nh <- length(hs)
# empty matrices
IMSE_LI <- matrix(NA, nrow=M, ncol=nh)
IMSE_LO <- matrix(NA, nrow=M, ncol=nh)
# generate matrix with M rows of sampled data
set.seed(22)
for(m in 1:M){
X <- dgp(n)
for(j in 1:nh){
temp <- imse(hs[j],X)
IMSE_LI[m,j] <- temp[1]
IMSE_LO[m,j] <- temp[2]
}
}
df2 <- data.frame(h = hs, leavein = colMeans(IMSE_LI), leaveout = colMeans(IMSE_LO)) %>%
gather(key = inout, value = imse, -h)
View(df2)
View(df2_10)
h_aimse <- h_aimse*2
K0 <- function(u){
out <- .75 * (1-u^2) * (abs(u) <= 1)
}
# function to calculate IMSE
imse <- function(h,X){
# empty vectors to fill with results
e_li <- rep(NA,n)
e_lo <- rep(NA,n)
# loop over each i to do leave one out
for(i in 1:n){
# repeat observation for each simulation
Xi_n <- rep(X[i], n)
# apply kernel function to (x-x_i)/h
df <- (1/h)*K0((Xi_n-X)/h)
# fhat with i in
fhat_li <- mean(df)
# fhat with i out
fhat_lo <- mean(df[-i])
# f(x_i)
f_xi <- f_true(X[i])
# mse with i in
e_li[i] <- (fhat_li-f_xi)^2
# mse with i out
e_lo[i] <- (fhat_lo-f_xi)^2
}
out <- c(mean(e_li),mean(e_lo))
return(out)
}
# simulate 1000 times
M <- 50
# sequence of h's to test
hs <- seq(.5,1.5,.1) * h_aimse
nh <- length(hs)
# empty matrices to fill
imse_li <- matrix(NA, nrow=M, ncol=nh)
imse_lo <- matrix(NA, nrow=M, ncol=nh)
# generate matrix with M rows of sampled data
set.seed(22)
for(m in 1:M){
X <- dgp(n)
for(j in 1:nh){
temp <- imse(hs[j],X)
imse_li[m,j] <- temp[1]
imse_lo[m,j] <- temp[2]
}
}
df <- data.frame(h = hs, leavein = colMeans(IMSE_LI), leaveout = colMeans(IMSE_LO)) %>%
gather(key = inout, value = imse, -h)
# plot
p <- ggplot(df2, aes(x=h,y=imse,color=inout)) + geom_smooth(se=F) +
theme_minimal()
p
p
View(df)
# plot
p <- ggplot(df, aes(x=h,y=imse,color=inout)) + geom_smooth(se=F) +
theme_minimal()
p
# theoretically optimal h
h_aimse <- optimal_h(1000,mu_true,sqrt(var_true))
?interagrate
?integrate
optimal_h <- function(n,mean,sd){
f <- function(x,mean,sd){norm_2d(x,mean,sd)^2}
k1 <- .75^2*(2-4/3+2/5)
k2 <- .75*(2/3-2/5)
k3 <- integrate(f, lower = -Inf, upper = Inf, mean = mean, sd = sd)$val
h <- (k1/(k3*k2^2)*(1/n))^(1/5)
return(h)
}
# theoretically optimal h
h_aimse <- optimal_h(1000,mu_true,sqrt(var_true))
###############################################################################
# Author: Paul R. Organ
# Purpose: ECON 675, PS2
# Last Update: Oct 8, 2018
###############################################################################
# Preliminaries
options(stringsAsFactors = F)
# packages
require(tidyverse) # data cleaning and manipulation
require(magrittr)  # syntax
require(ggplot2)   # plots
require(kedd)      # kernel bandwidth estimation
require(car)       # heteroskedastic robust SEs
require(xtable)    # tables for LaTeX
setwd('C:/Users/prorgan/Box/Classes/Econ 675/Problem Sets/PS2')
###############################################################################
## Question 1: Kernel Density Estimation
## Q1.3a
# sample size
n     <- 1000
# data generating process
dgp <- function(n){
# equally weight two distributions
comps <- sample(1:2,prob=c(.5,.5),size=n,replace=T)
# Normal density specs
mus <- c(-1.5, 1)
sds <- sqrt(c(1.5, 1))
# generate sample
samp <- rnorm(n=n,mean=mus[comps],sd=sds[comps])
return(samp)
}
# mean of mixture
mu_true <- .5*-1.5 + .5*1
# sd of mixture
var_true <- .5*1.5 + .5*1 + (.5*-1.5+.5*1-(.5*-1.5+.5*1)^2)
# see: https://stats.stackexchange.com/questions/16608/
# what-is-the-variance-of-the-weighted-mixture-of-two-gaussians
# true dgp
f_true <- function(x){dnorm(x,mean=mu_true,sd=sqrt(var_true))}
# second deriv of normal dist
norm_2d <- function(u,meanu,sdu){dnorm(u,mean=meanu,sd=sdu)*
(((u-meanu)^2/(sdu^4))-(1/(sdu^2)))}
# AIMSE-optimal bandwidth choice
optimal_h <- function(n,mean,sd){
f <- function(x,mean,sd){norm_2d(x,mean,sd)^2}
k1 <- .75^2*(2-4/3+2/5)
k2 <- .75*(2/3-2/5)
k3 <- integrate(f, lower = -Inf, upper = Inf, mean = mean, sd = sd)$val
h <- (k1/(k3*k2^2)*(1/n))^(1/5)
return(h)
}
# theoretically optimal h
h_aimse <- optimal_h(1000,mu_true,sqrt(var_true))
###############################################################################
## Q1.3b
# define Kernel function: K(u)=.75(1-u^2)(ind(abs(u)<=1))
K0 <- function(u){
out <- .75 * (1-u^2) * (abs(u) <= 1)
}
# function to calculate IMSE
imse <- function(h,X){
# empty vectors to fill with results
e_li <- rep(NA,n)
e_lo <- rep(NA,n)
# loop over each i to do leave one out
for(i in 1:n){
# repeat observation for each simulation
Xi_n <- rep(X[i], n)
# apply kernel function to (x-x_i)/h
df <- (1/h)*K0((Xi_n-X)/h)
# fhat with i in
fhat_li <- mean(df)
# fhat with i out
fhat_lo <- mean(df[-i])
# f(x_i)
f_xi <- f_true(X[i])
# mse with i in
e_li[i] <- (fhat_li-f_xi)^2
# mse with i out
e_lo[i] <- (fhat_lo-f_xi)^2
}
out <- c(mean(e_li),mean(e_lo))
return(out)
}
# simulate 1000 times
M <- 1000
# sequence of h's to test
hs <- seq(.5,1.5,.1) * h_aimse
nh <- length(hs)
# empty matrices to fill
imse_li <- matrix(NA, nrow=M, ncol=nh)
imse_lo <- matrix(NA, nrow=M, ncol=nh)
# generate matrix with M rows of sampled data
set.seed(22)
for(m in 1:M){
X <- dgp(n)
for(j in 1:nh){
temp <- imse(hs[j],X)
imse_li[m,j] <- temp[1]
imse_lo[m,j] <- temp[2]
}
}
df <- data.frame(h = hs, leavein = colMeans(IMSE_LI), leaveout = colMeans(IMSE_LO)) %>%
gather(key = inout, value = imse, -h)
# plot
p <- ggplot(df, aes(x=h,y=imse,color=inout)) + geom_smooth(se=F) +
theme_minimal()
p
ggsave('q1_3b_R.png')
df <- data.frame(h = hs, leavein = colMeans(imse_li), leaveout = colMeans(imse_lo)) %>%
gather(key = inout, value = imse, -h)
# plot
p <- ggplot(df, aes(x=h,y=imse,color=inout)) + geom_smooth(se=F) +
theme_minimal()
p
ggsave('q1_3b_R.png')
opt_hs <- rep(NA,M)
set.seed(22)
for(m in 1:M){
X <- dgp(n)
mu <- mean(X)
sd <- sd(X)
hhat <- optimal_h(n,mu,sd)
}
hbar <- mean(opt_hs)
opt_hs <- rep(NA,M)
set.seed(22)
for(m in 1:M){
X <- dgp(n)
mu <- mean(X)
sd <- sd(X)
opt_hs[m] <- optimal_h(n,mu,sd)
}
hbar <- mean(opt_hs)
