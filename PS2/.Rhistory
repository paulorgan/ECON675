imse <- function(h,X){
# empty vectors to fill with results
e_li <- rep(NA,n)
e_lo <- rep(NA,n)
# loop over each i to do leave one out
for(i in 1:n){
# repeat observation for each simulation
Xi_n <- rep(X[i], n)
# apply kernel function to (x-x_i)/h
df <- (1/h)*K0((Xi_n-X)/h)
# fhat with i in
fhat_li <- mean(df)
# fhat with i out
fhat_lo <- mean(df[-i])
# f(x_i)
f_xi <- f_true(X[i])
# mse with i in
e_li[i] <- (fhat_li-f_xi)^2
# mse with i out
e_lo[i] <- (fhat_lo-f_xi)^2
}
out <- c(mean(e_li),mean(e_lo))
return(out)
}
# simulate 1000 times
M <- 1000
# sequence of h's to test
hs <- seq(.5,1.5,.1) * h_aimse
nh <- length(hs)
# empty matrices to fill
imse_li <- matrix(NA, nrow=M, ncol=nh)
imse_lo <- matrix(NA, nrow=M, ncol=nh)
set.seed(22)
for(m in 1:M){
X <- dgp(n)
for(j in 1:nh){
temp <- imse(hs[j],X)
imse_li[m,j] <- temp[1]
imse_lo[m,j] <- temp[2]
}
}
df <- data.frame(h = hs, leavein = colMeans(imse_li), leaveout = colMeans(imse_lo)) %>%
gather(key = inout, value = imse, -h)
p <- ggplot(df, aes(x=h,y=imse,color=inout)) + geom_smooth(se=F) +
theme_minimal()
p
ggsave('q1_3b_R.png')
optimal_h <- function(x,mu,sd){
if(x='theoretical'){
f <- f_int(x)
} else{
f <- norm_2d(x,mu,sd)^2
}
k1 <- .75^2 * (2- 4/3 + 2/5)
k2 <- .75 * (2/3 - 2/5)
k3 <- integrate(f,-Inf,Inf)
h <- (k1/(k3*k2^2)*(1/n))^(1/5)
return(h)
}
optimal_h <- function(x,mu,sd){
if(x='theoretical'){
f <- f_int(x)
} else{
f <- norm_2d(x,mu,sd)^2
}
k1 <- .75^2 * (2- 4/3 + 2/5)
k2 <- .75 * (2/3 - 2/5)
k3 <- integrate(f,-Inf,Inf)
h <- (k1/(k3*k2^2)*(1/n))^(1/5)
return(h)
}
optimal_h <- function(x,mu,sd){
if(x=='theoretical'){
f <- f_int(x)
} else{
f <- norm_2d(x,mu,sd)^2
}
k1 <- .75^2 * (2- 4/3 + 2/5)
k2 <- .75 * (2/3 - 2/5)
k3 <- integrate(f,-Inf,Inf)
h <- (k1/(k3*k2^2)*(1/n))^(1/5)
return(h)
}
# theoretically optimal h
h_aimse <- ( (3/5) / (v2k * (1/5)^2 * 1000) ) ^ (1/5)
optimal_h('theoretical',NA,NA)
optimal_h <- function(x,mu,sd){
if(x=='theoretical'){
f <- f_int
} else{
f <- function(x,mu,sd){norm_2d(x,mu,sd)^2}
}
k1 <- .75^2 * (2- 4/3 + 2/5)
k2 <- .75 * (2/3 - 2/5)
k3 <- integrate(f,-Inf,Inf)
h <- (k1/(k3*k2^2)*(1/n))^(1/5)
return(h)
}
optimal_h('theoretical',NA,NA)
k1 <- .75^2 * (2- 4/3 + 2/5)
k2 <- .75 * (2/3 - 2/5)
k3 <- integrate(f,-Inf,Inf)
f <- f_int
k3 <- integrate(f,-Inf,Inf)$val
h <- (k1/(k3*k2^2)*(1/n))^(1/5)
optimal_h <- function(x,mu,sd){
if(x=='theoretical'){
f <- f_int
} else{
f <- function(x,mu,sd){norm_2d(x,mu,sd)^2}
}
k1 <- .75^2 * (2- 4/3 + 2/5)
k2 <- .75 * (2/3 - 2/5)
k3 <- integrate(f,-Inf,Inf)$val
h <- (k1/(k3*k2^2)*(1/n))^(1/5)
return(h)
}
optimal_h('theoretical',NA,NA)
# theoretically optimal h
h_aimse <- optimal_h('theoretical',NA,NA)
opt_hs <- rep(NA,M)
set.seed(22)
for(m in 1:M){
X <- dgp(n)
mu <- mean(X)
sd <- sd(X)
opt_hs[m] <- optimal_h(n,mu,sd)
}
hbar <- mean(opt_hs)
optimal_h <- function(x,mu,sd){
if(x=='theoretical'){
f <- f_int
} else{
f <- function(x,mu,sd){norm_2d(x,mu,sd)^2}
}
k1 <- .75^2 * (2- 4/3 + 2/5)
k2 <- .75 * (2/3 - 2/5)
k3 <- integrate(f,-Inf,Inf,mu=mu,sd=sd)$val
h <- (k1/(k3*k2^2)*(1/n))^(1/5)
return(h)
}
# theoretically optimal h
h_aimse <- optimal_h('theoretical',NA,NA)
h_aimse<-NULL
# theoretically optimal h
h_aimse <- optimal_h('theoretical',NA,NA)
optimal_h <- function(x,mu,sd){
k1 <- .75^2 * (2- 4/3 + 2/5)
k2 <- .75 * (2/3 - 2/5)
if(x=='theoretical'){
f <- f_int
k3 <- integrate(f,-Inf,Inf)$val
} else{
f <- function(x,mu,sd){norm_2d(x,mu,sd)^2}
k3 <- integrate(f,-Inf,Inf,mu=mu,sd=sd)$val
}
h <- (k1/(k3*k2^2)*(1/n))^(1/5)
return(h)
}
# theoretically optimal h
h_aimse <- optimal_h('theoretical',NA,NA)
opt_hs <- rep(NA,M)
set.seed(22)
for(m in 1:M){
X <- dgp(n)
mu <- mean(X)
sd <- sd(X)
opt_hs[m] <- optimal_h(n,mu,sd)
}
hbar <- mean(opt_hs)
imse <- function(h,X){
# empty vectors to fill with results
e_li <- rep(NA,n)
e_lo <- rep(NA,n)
# loop over each i to do leave one out
for(i in 1:n){
# repeat observation for each simulation
Xi_n <- rep(X[i], n)
# apply kernel function to (x-x_i)/h
df <- K0((Xi_n-X)/h)
# fhat with i in
fhat_li <- mean(df)/h
# fhat with i out
fhat_lo <- mean(df[-i])/h
# f(x_i)
f_xi <- f_true(X[i])
# mse with i in
e_li[i] <- (fhat_li-f_xi)^2
# mse with i out
e_lo[i] <- (fhat_lo-f_xi)^2
}
out <- c(mean(e_li),mean(e_lo))
return(out)
}
###############################################################################
# Author: Paul R. Organ
# Purpose: ECON 675, PS2
# Last Update: Oct 9, 2018
###############################################################################
# Preliminaries
options(stringsAsFactors = F)
# packages
require(tidyverse) # data cleaning and manipulation
require(magrittr)  # syntax
require(ggplot2)   # plots
require(kedd)      # kernel bandwidth estimation
require(car)       # heteroskedastic robust SEs
require(xtable)    # tables for LaTeX
setwd('C:/Users/prorgan/Box/Classes/Econ 675/Problem Sets/PS2')
###############################################################################
## Question 1: Kernel Density Estimation
## Q1.3a
# sample size
n     <- 1000
# data generating process
dgp <- function(n){
# equally weight two distributions
comps <- sample(1:2,prob=c(.5,.5),size=n,replace=T)
# Normal density specs
mus <- c(-1.5, 1)
sds <- sqrt(c(1.5, 1))
# generate sample
samp <- rnorm(n=n,mean=mus[comps],sd=sds[comps])
return(samp)
}
# mean of mixture
mu_true <- .5*-1.5 + .5*1
# sd of mixture
var_true <- .5*1.5 + .5*1 + (.5*-1.5+.5*1-(.5*-1.5+.5*1)^2)
# see: https://stats.stackexchange.com/questions/16608/
# what-is-the-variance-of-the-weighted-mixture-of-two-gaussians
# true dgp
f_true <- function(x){dnorm(x,mean=mu_true,sd=sqrt(var_true))}
# second deriv of normal dist
norm_2d <- function(u,meanu,sdu){dnorm(u,mean=meanu,sd=sdu)*
(((u-meanu)^2/(sdu^4))-(1/(sdu^2)))}
# f for integration (theoretical)
f_int <- function(x){
return( (.5*norm_2d(x,-1.5,sqrt(1.5)) + .5*norm_2d(x,1,1))^2 )
}
optimal_h <- function(x,mu,sd){
k1 <- .75^2 * (2- 4/3 + 2/5)
k2 <- .75 * (2/3 - 2/5)
if(x=='theoretical'){
f <- f_int
k3 <- integrate(f,-Inf,Inf)$val
} else{
f <- function(x,mu,sd){norm_2d(x,mu,sd)^2}
k3 <- integrate(f,-Inf,Inf,mu=mu,sd=sd)$val
}
h <- (k1/(k3*k2^2)*(1/n))^(1/5)
return(h)
}
# theoretically optimal h
h_aimse <- optimal_h('theoretical',NA,NA)
###############################################################################
## Q1.3b
# define Kernel function: K(u)=.75(1-u^2)(ind(abs(u)<=1))
K0 <- function(u){
out <- .75 * (1-u^2) * (abs(u) <= 1)
}
# function to calculate IMSE
imse <- function(h,X){
# empty vectors to fill with results
e_li <- rep(NA,n)
e_lo <- rep(NA,n)
# loop over each i to do leave one out
for(i in 1:n){
# repeat observation for each simulation
Xi_n <- rep(X[i], n)
# apply kernel function to (x-x_i)/h
df <- K0((Xi_n-X)/h)
# fhat with i in
fhat_li <- mean(df)/h
# fhat with i out
fhat_lo <- mean(df[-i])/h
# f(x_i)
f_xi <- f_true(X[i])
# mse with i in
e_li[i] <- (fhat_li-f_xi)^2
# mse with i out
e_lo[i] <- (fhat_lo-f_xi)^2
}
out <- c(mean(e_li),mean(e_lo))
return(out)
}
# simulate 1000 times
M <- 25
hs <- seq(.5,1.5,.1) * h_aimse
nh <- length(hs)
set.seed(22)
for(m in 1:M){
X <- dgp(n)
for(j in 1:nh){
temp <- imse(hs[j],X)
imse_li[m,j] <- temp[1]
imse_lo[m,j] <- temp[2]
}
}
df <- data.frame(h = hs, leavein = colMeans(imse_li), leaveout = colMeans(imse_lo)) %>%
gather(key = inout, value = imse, -h)
# plot
p <- ggplot(df, aes(x=h,y=imse,color=inout)) + geom_smooth(se=F) +
theme_minimal()
p
imse_li <- matrix(NA, nrow=M, ncol=nh)
imse_lo <- matrix(NA, nrow=M, ncol=nh)
# generate matrix with M rows of sampled data
set.seed(22)
for(m in 1:M){
X <- dgp(n)
for(j in 1:nh){
temp <- imse(hs[j],X)
imse_li[m,j] <- temp[1]
imse_lo[m,j] <- temp[2]
}
}
df <- data.frame(h = hs, leavein = colMeans(imse_li), leaveout = colMeans(imse_lo)) %>%
gather(key = inout, value = imse, -h)
# plot
p <- ggplot(df, aes(x=h,y=imse,color=inout)) + geom_smooth(se=F) +
theme_minimal()
p
View(df)
View(imse_li)
# true dgp
f_true <- function(x){.5*dnorm(x,-1.5,1.5)+.5*dnorm(x,1,1)}
K0 <- function(u){
out <- .75 * (1-u^2) * (abs(u) <= 1)
}
# function to calculate IMSE
imse <- function(h,X){
# empty vectors to fill with results
e_li <- rep(NA,n)
e_lo <- rep(NA,n)
# loop over each i to do leave one out
for(i in 1:n){
# repeat observation for each simulation
Xi_n <- rep(X[i], n)
# apply kernel function to (x-x_i)/h
df <- K0((Xi_n-X)/h)
# fhat with i in
fhat_li <- mean(df)/h
# fhat with i out
fhat_lo <- mean(df[-i])/h
# f(x_i)
f_xi <- f_true(X[i])
# mse with i in
e_li[i] <- (fhat_li-f_xi)^2
# mse with i out
e_lo[i] <- (fhat_lo-f_xi)^2
}
out <- c(mean(e_li),mean(e_lo))
return(out)
}
# simulate 1000 times
M <- 25
# sequence of h's to test
hs <- seq(.5,1.5,.1) * h_aimse
nh <- length(hs)
# empty matrices to fill
imse_li <- matrix(NA, nrow=M, ncol=nh)
imse_lo <- matrix(NA, nrow=M, ncol=nh)
# generate matrix with M rows of sampled data
set.seed(22)
for(m in 1:M){
X <- dgp(n)
for(j in 1:nh){
temp <- imse(hs[j],X)
imse_li[m,j] <- temp[1]
imse_lo[m,j] <- temp[2]
}
}
df <- data.frame(h = hs, leavein = colMeans(imse_li), leaveout = colMeans(imse_lo)) %>%
gather(key = inout, value = imse, -h)
# plot
p <- ggplot(df, aes(x=h,y=imse,color=inout)) + geom_smooth(se=F) +
theme_minimal()
p
###############################################################################
# Author: Paul R. Organ
# Purpose: ECON 675, PS2
# Last Update: Oct 9, 2018
###############################################################################
# Preliminaries
options(stringsAsFactors = F)
# packages
require(tidyverse) # data cleaning and manipulation
require(magrittr)  # syntax
require(ggplot2)   # plots
require(kedd)      # kernel bandwidth estimation
require(car)       # heteroskedastic robust SEs
require(xtable)    # tables for LaTeX
setwd('C:/Users/prorgan/Box/Classes/Econ 675/Problem Sets/PS2')
###############################################################################
## Question 1: Kernel Density Estimation
## Q1.3a
# sample size
n     <- 1000
# data generating process
dgp <- function(n){
# equally weight two distributions
comps <- sample(1:2,prob=c(.5,.5),size=n,replace=T)
# Normal density specs
mus <- c(-1.5, 1)
sds <- sqrt(c(1.5, 1))
# generate sample
samp <- rnorm(n=n,mean=mus[comps],sd=sds[comps])
return(samp)
}
# mean of mixture
mu_true <- .5*-1.5 + .5*1
# sd of mixture
var_true <- .5*1.5 + .5*1 + (.5*-1.5+.5*1-(.5*-1.5+.5*1)^2)
# see: https://stats.stackexchange.com/questions/16608/
# what-is-the-variance-of-the-weighted-mixture-of-two-gaussians
# true dgp
f_true <- function(x){dnorm(x,mean=mu_true,sd=sqrt(var_true))}
# true dgp
f_true <- function(x){.5*dnorm(x,-1.5,1.5)+.5*dnorm(x,1,1)}
# second deriv of normal dist
norm_2d <- function(u,meanu,sdu){dnorm(u,mean=meanu,sd=sdu)*
(((u-meanu)^2/(sdu^4))-(1/(sdu^2)))}
# f for integration (theoretical)
f_int <- function(x){
return( (.5*norm_2d(x,-1.5,sqrt(1.5)) + .5*norm_2d(x,1,1))^2 )
}
optimal_h <- function(x,mu,sd){
k1 <- .75^2 * (2- 4/3 + 2/5)
k2 <- .75 * (2/3 - 2/5)
if(x=='theoretical'){
f <- f_int
k3 <- integrate(f,-Inf,Inf)$val
} else{
f <- function(x,mu,sd){norm_2d(x,mu,sd)^2}
k3 <- integrate(f,-Inf,Inf,mu=mu,sd=sd)$val
}
h <- (k1/(k3*k2^2)*(1/n))^(1/5)
return(h)
}
# theoretically optimal h
h_aimse <- optimal_h('theoretical',NA,NA)
###############################################################################
## Q1.3b
# define Kernel function: K(u)=.75(1-u^2)(ind(abs(u)<=1))
K0 <- function(u){
out <- .75 * (1-u^2) * (abs(u) <= 1)
}
# function to calculate IMSE
imse <- function(h,X){
# empty vectors to fill with results
e_li <- rep(NA,n)
e_lo <- rep(NA,n)
# loop over each i to do leave one out
for(i in 1:n){
# repeat observation for each simulation
Xi_n <- rep(X[i], n)
# apply kernel function to (x-x_i)/h
df <- K0((Xi_n-X)/h)
# fhat with i in
fhat_li <- mean(df)/h
# fhat with i out
fhat_lo <- mean(df[-i])/h
# f(x_i)
f_xi <- f_true(X[i])
# mse with i in
e_li[i] <- (fhat_li-f_xi)^2
# mse with i out
e_lo[i] <- (fhat_lo-f_xi)^2
}
out <- c(mean(e_li),mean(e_lo))
return(out)
}
# simulate 1000 times
M <- 1000
# sequence of h's to test
hs <- seq(.5,1.5,.1) * h_aimse
nh <- length(hs)
# empty matrices to fill
imse_li <- matrix(NA, nrow=M, ncol=nh)
imse_lo <- matrix(NA, nrow=M, ncol=nh)
# generate matrix with M rows of sampled data
set.seed(22)
for(m in 1:M){
X <- dgp(n)
for(j in 1:nh){
temp <- imse(hs[j],X)
imse_li[m,j] <- temp[1]
imse_lo[m,j] <- temp[2]
}
}
df <- data.frame(h = hs, leavein = colMeans(imse_li), leaveout = colMeans(imse_lo)) %>%
gather(key = inout, value = imse, -h)
# plot
p <- ggplot(df, aes(x=h,y=imse,color=inout)) + geom_smooth(se=F) +
theme_minimal()
p
ggsave('q1_3b_R.png')
###############################################################################
## Q1.3d
opt_hs <- rep(NA,M)
set.seed(22)
for(m in 1:M){
X <- dgp(n)
mu <- mean(X)
sd <- sd(X)
opt_hs[m] <- optimal_h(n,mu,sd)
}
hbar <- mean(opt_hs)
# averages
h_hat_li <- df %>% filter(inout == 'leavein') %>% filter(imse == min(imse)) %>% select(h) %>% as.numeric
h_hat_lo <- df %>% filter(inout == 'leaveout') %>% filter(imse == min(imse)) %>% select(h) %>% as.numeric
